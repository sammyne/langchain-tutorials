{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62faa8ba",
   "metadata": {},
   "source": [
    "# Build a RAG agent with LangChain\n",
    "\n",
    "## Overview\n",
    "\n",
    "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q\\&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](https://docs.langchain.com/oss/python/langchain/retrieval/).\n",
    "\n",
    "This tutorial will show how to build a simple Q\\&A application over an unstructured text data source. We will demonstrate:\n",
    "\n",
    "1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n",
    "2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "We will cover the following concepts:\n",
    "\n",
    "* **Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens in a separate process.*\n",
    "\n",
    "* **Retrieval and generation**: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "Once we've indexed our data, we will use an [agent](https://docs.langchain.com/oss/python/langchain/agents) as our orchestration framework to implement the retrieval and generation steps.\n",
    "\n",
    "> The indexing portion of this tutorial will largely follow the [semantic search tutorial](https://docs.langchain.com/oss/python/langchain/knowledge-base).\n",
    "> \n",
    "> If your data is already available for search (i.e., you have a function to execute a search), or you're comfortable with the content from that tutorial, feel free to skip to the section on [retrieval and generation](#2-retrieval-and-generation)\n",
    "\n",
    "### Preview\n",
    "\n",
    "In this guide we'll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n",
    "\n",
    "We can create a simple indexing pipeline and RAG chain to do this in \\~40 lines of code. See below for the full code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ff56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8eddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain.agents import AgentState, create_agent\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.messages import MessageLikeRepresentation\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c3457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7036442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tool for retrieving context\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9669f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Must use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9619882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_0908786027974354b684c2)\n",
      " Call ID: call_0908786027974354b684c2\n",
      "  Args:\n",
      "    query: task decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks or steps. It is a key component of planning in AI agents and helps improve performance on intricate problems. Common approaches include:\n",
      "\n",
      "- **LLM-based decomposition**, using simple prompting techniques like:\n",
      "  - “Steps for XYZ.\\n1.”\n",
      "  - “What are the subgoals for achieving XYZ?”\n",
      "- **Task-specific instructions**, such as “Write a story outline” when generating a novel.\n",
      "- **Human input**, where domain experts manually define subtasks.\n",
      "- **LLM+P (LLM + classical planner)**, which outsources planning to an external tool using Planning Domain Definition Language (PDDL):\n",
      "  1. LLM translates the problem into *Problem PDDL*,\n",
      "  2. A classical planner generates a plan using a pre-defined *Domain PDDL*,\n",
      "  3. LLM translates the PDDL plan back into natural language.\n",
      "\n",
      "Related reasoning techniques include:\n",
      "- **Chain of Thought (CoT)**: Encourages step-by-step reasoning to decompose tasks interpretably.\n",
      "- **Tree of Thoughts (ToT)**: Extends CoT by exploring multiple reasoning paths per step—forming a tree—and using search (BFS/DFS) with evaluation (e.g., via prompts or voting) to guide selection.\n",
      "\n",
      "These methods enhance reasoning, scalability, and transparency in agent systems.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525a298",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Installation\n",
    "\n",
    "This tutorial requires these langchain dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d82eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-community~=0.4 langchain-chroma~=1.0 langchain[openai]~=1.0 beautifulsoup4~=4.14 python-dotenv~=1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada80259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 CPU 版 PyTorch，避免 sentence-transformers 自动安装 GPU 版 PyTorch\n",
    "%uv pip install torch~=2.9 --index-url https://download.pytorch.org/whl/cpu\n",
    "%uv pip install langchain-huggingface~=1.2 sentence-transformers~=5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ca33f",
   "metadata": {},
   "source": [
    "For more details, see our [Installation guide](https://docs.langchain.com/oss/python/langchain/install).\n",
    "\n",
    "### Components\n",
    "We will need to select three components from LangChain’s suite of integrations.\n",
    "Select a chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "345af79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE_URL\"],\n",
    "    model=os.environ[\"OPENAI_MODEL\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba90e28",
   "metadata": {},
   "source": [
    "\n",
    "Select an embeddings model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a66cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "# model_name='BAAI/bge-small-en-v1.5'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8182e0",
   "metadata": {},
   "source": [
    "Select a vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcedff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./_rag-agent-db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d572f",
   "metadata": {},
   "source": [
    "## 1. Indexing\n",
    "\n",
    "> **This section is an abbreviated version of the content in the [semantic search tutorial](https://docs.langchain.com/oss/python/langchain/knowledge-base).**\n",
    "> \n",
    "> If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you're comfortable with [document loaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders), [embeddings](https://docs.langchain.com/oss/python/langchain/retrieval#embedding_models), and [vector stores](https://docs.langchain.com/oss/python/langchain/retrieval#vectorstores), feel free to skip to the next section on [retrieval and generation](https://docs.langchain.com/oss/python/langchain/rag#2-retrieval-and-generation).\n",
    "\n",
    "Indexing commonly works as follows:\n",
    "\n",
    "1. **Load**: First we need to load our data. This is done with [Document Loaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders).\n",
    "2. **Split**: [Text splitters](https://docs.langchain.com/oss/python/langchain/retrieval#text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://docs.langchain.com/oss/python/langchain/retrieval#vectorstores) and [Embeddings](https://docs.langchain.com/oss/python/langchain/retrieval#embedding_models) model.\n",
    "\n",
    "<img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=21403ce0d0c772da84dcc5b75cff4451\" alt=\"index_diagram\" data-og-width=\"2583\" width=\"2583\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_indexing.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=bf4eb8255b82a809dbbd2bc2a96d2ed7 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4ebc538b2c4765b609f416025e4dbbda 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=1838328a870c7353c42bf1cc2290a779 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=675f55e100bab5e2904d27db01775ccc 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4b9e544a7a3ec168651558bce854eb60 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=f5aeaaaea103128f374c03b05a317263 2500w\" />\n",
    "\n",
    "### Loading documents\n",
    "\n",
    "We need to first load the blog post contents. We can use [DocumentLoaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders) for this, which are objects that load in data from a source and return a list of [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.\n",
    "\n",
    "In this case we'll use the [`WebBaseLoader`](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we'll remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23a8cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e08e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71359d53",
   "metadata": {},
   "source": [
    "**Go deeper**\n",
    "\n",
    "`DocumentLoader`: Object that loads data from a source as list of `Documents`.\n",
    "\n",
    "* [Integrations](https://docs.langchain.com/oss/python/integrations/document_loaders/): 160+ integrations to choose from.\n",
    "* [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader): API reference for the base interface.\n",
    "\n",
    "### Splitting documents\n",
    "\n",
    "Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we'll split the [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n",
    "\n",
    "As in the [semantic search tutorial](https://docs.langchain.com/oss/python/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa57877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59091b14",
   "metadata": {},
   "source": [
    "**Go deeper**\n",
    "\n",
    "`TextSplitter`: Object that splits a list of [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects into smaller\n",
    "chunks for storage and retrieval.\n",
    "\n",
    "* [Integrations](https://docs.langchain.com/oss/python/integrations/splitters/)\n",
    "* [Interface](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html): API reference for the base interface.\n",
    "\n",
    "### Storing documents\n",
    "\n",
    "Now we need to index our 66 text chunks so that we can search over them at runtime. Following the [semantic search tutorial](https://docs.langchain.com/oss/python/langchain/knowledge-base), our approach is to [embed](https://docs.langchain.com/oss/python/langchain/retrieval#embedding_models/) the contents of each document split and insert these embeddings into a [vector store](https://docs.langchain.com/oss/python/langchain/retrieval#vectorstores/). Given an input query, we can then use vector search to retrieve relevant documents.\n",
    "\n",
    "We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the [start of the tutorial](https://docs.langchain.com/oss/python/langchain/rag#components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "023706d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d9c1174e-8fc7-4448-b8b8-916ef7af93a7', '2ef5b287-eba7-4042-b852-acbc69a8b23f', '87057d97-95f2-4529-9952-ab5fde00cfcc']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af286770",
   "metadata": {},
   "source": [
    "**Go deeper**\n",
    "\n",
    "`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.\n",
    "\n",
    "* [Integrations](https://docs.langchain.com/oss/python/integrations/text_embedding/): 30+ integrations to choose from.\n",
    "* [Interface](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings): API reference for the base interface.\n",
    "\n",
    "`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.\n",
    "\n",
    "* [Integrations](https://docs.langchain.com/oss/python/integrations/vectorstores/): 40+ integrations to choose from.\n",
    "* [Interface](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html): API reference for the base interface.\n",
    "\n",
    "This completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n",
    "\n",
    "## 2. Retrieval and Generation\n",
    "\n",
    "RAG applications commonly work as follows:\n",
    "\n",
    "1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://docs.langchain.com/oss/python/langchain/retrieval#retrievers).\n",
    "2. **Generate**: A [model](https://docs.langchain.com/oss/python/langchain/models) produces an answer using a prompt that includes both the question with the retrieved data\n",
    "\n",
    "<img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=994c3585cece93c80873d369960afd44\" alt=\"retrieval_diagram\" data-og-width=\"2532\" width=\"2532\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_retrieval_generation.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=3bd28b3662e08c8364b60b74f510751e 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=43484903ca631a47a54e86191eb5ba22 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=67fe2302e241fc24238a5df1cf56573d 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=d390a6a758e688ec36352d30b22249b0 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=59729377317a0631598b6a4a2a7d8c92 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=c07711c71153c3b2dfd5b0104ad3e324 2500w\" />\n",
    "\n",
    "Now let's write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
    "\n",
    "We will demonstrate:\n",
    "\n",
    "1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n",
    "2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n",
    "\n",
    "### RAG agents\n",
    "\n",
    "One formulation of a RAG application is as a simple [agent](https://docs.langchain.com/oss/python/langchain/agents) with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a [tool](https://docs.langchain.com/oss/python/langchain/tools) that wraps our vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7035ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec48c9",
   "metadata": {},
   "source": [
    "> Here we use the [tool decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) to configure the tool to attach raw documents as [artifacts](https://docs.langchain.com/oss/python/langchain/messages#param-artifact) to each [ToolMessage](https://docs.langchain.com/oss/python/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n",
    "\n",
    ">  Retrieval tools are not limited to a single string `query` argument, as in the above example. You can\n",
    ">  force the LLM to specify additional search parameters by adding arguments— for example, a category:\n",
    ">\n",
    ">  ```python  theme={null}\n",
    ">  from typing import Literal\n",
    ">\n",
    ">  def retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n",
    ">  ```\n",
    "\n",
    "Given our tool, we can construct the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d049c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f7e12",
   "metadata": {},
   "source": [
    "Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f81b5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_f330fa8b956746d4acebc8)\n",
      " Call ID: call_f330fa8b956746d4acebc8\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition is **Chain of Thought (CoT)**, introduced by Wei et al. (2022). It involves prompting the LLM to “think step by step”, thereby decomposing complex tasks into smaller, manageable sub-steps—enhancing reasoning and interpretability.\n",
      "\n",
      "Common extensions of CoT include:\n",
      "\n",
      "- **Tree of Thoughts (ToT)** (Yao et al., 2023): Extends CoT by generating *multiple candidate thoughts* at each reasoning step, organizing them into a tree structure. It employs search strategies (e.g., BFS or DFS) and uses prompt-based evaluation or majority voting to prune and select promising paths.\n",
      "\n",
      "- **LLM+P (LLM + Classical Planning)** (Liu et al., 2023): A hybrid approach where the LLM interfaces with an external classical planner (using PDDL) to handle long-horizon, structured planning—offloading formal reasoning to domain-specific planners.\n",
      "\n",
      "- **Task-specific decomposition prompts**: Using tailored instructions (e.g., “Write a story outline”) or human-guided decomposition, which complements or substitutes generic CoT depending on domain needs.\n",
      "\n",
      "Let me know if you'd like deeper explanations or comparisons of these extensions.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563cf78",
   "metadata": {},
   "source": [
    "Note that the agent:\n",
    "\n",
    "1. Generates a query to search for a standard method for task decomposition;\n",
    "2. Receiving the answer, generates a second query to search for common extensions of it;\n",
    "3. Having received all necessary context, answers the question.\n",
    "\n",
    "We can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).\n",
    "\n",
    "> You can add a deeper level of control and customization using the [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview) framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](https://docs.langchain.com/oss/python/langgraph/agentic-rag) for more advanced formulations.\n",
    "\n",
    "### RAG chains\n",
    "\n",
    "In the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](https://docs.langchain.com/oss/python/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n",
    "\n",
    "| ✅ Benefits                                                                                                                                                 | ⚠️ Drawbacks                                                                                                                                |\n",
    "| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Search only when needed** – The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.                        | **Two inference calls** – When a search is performed, it requires one call to generate the query and another to produce the final response. |\n",
    "| **Contextual search queries** – By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** – The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.                    |\n",
    "| **Multiple searches allowed** – The LLM can execute several searches in support of a single user query.                                                    |                                                                                                                                             |\n",
    "\n",
    "Another common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\n",
    "\n",
    "In this approach we no longer call the model in a loop, but instead make a single pass.\n",
    "\n",
    "We can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5aaeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4490a1",
   "metadata": {},
   "source": [
    "Let's try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "886f5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks or steps. This enables better planning, reasoning, and execution—especially for AI agents or humans tackling multi-step problems.\n",
      "\n",
      "In AI systems, task decomposition can be achieved in several ways:\n",
      "\n",
      "1. **LLM-based decomposition via prompting**:  \n",
      "   - Simple prompts like *“Steps for XYZ.\\n1.”* or *“What are the subgoals for achieving XYZ?”* guide the LLM to generate an ordered list of subtasks.  \n",
      "   - More advanced techniques include **Chain-of-Thought (CoT)** (Wei et al., 2022), where the model is prompted to “think step by step”, explicitly revealing its reasoning path; and **Tree-of-Thoughts (ToT)** (Yao et al., 2023), which explores *multiple possible reasoning paths* at each step—forming a search tree evaluated via prompting or voting.\n",
      "\n",
      "2. **Task-specific instructions**:  \n",
      "   - Using domain-aware directives, e.g., *“Write a story outline.”* for creative writing, or *“Generate a SQL query to find top-5 customers by revenue.”* — these implicitly constrain and structure the decomposition.\n",
      "\n",
      "3. **Human-in-the-loop decomposition**:  \n",
      "   - A person manually defines or refines subtasks, especially when precision, domain expertise, or safety-critical reasoning is required.\n",
      "\n",
      "4. **LLM+P (LLM + Classical Planner)**:  \n",
      "   - As proposed by Liu et al. (2023), this hybrid approach outsources long-horizon, logic-heavy planning to an external classical planner (e.g., Fast-Downward). The LLM acts as an interface:  \n",
      "     (i) Translates the natural language problem into a **Problem PDDL**,  \n",
      "     (ii) Uses a pre-defined **Domain PDDL** (encoding actions, preconditions, effects) to request a plan from the planner,  \n",
      "     (iii) Converts the resulting PDDL plan back into natural language.  \n",
      "   - This is powerful in structured domains (e.g., robotics, logistics) where formal planning models exist—but less applicable where domain PDDLs are unavailable or hard to specify.\n",
      "\n",
      "In summary, task decomposition is foundational to intelligent task execution—it bridges high-level goals with concrete, executable actions—and its method depends on the complexity of the task, available tools, domain structure, and desired reliability.\n",
      "\n",
      "User Input: What is task decomposition?  \n",
      "Task Planning: Explain definition, list and compare major approaches (LLM prompting, task-specific instructions, human input, LLM+P), highlight key papers (CoT, ToT, Liu et al. 2023), and clarify distinctions (e.g., CoT vs. ToT vs. PDDL-based planning).  \n",
      "Model Selection: I used my internal multimodal reasoning architecture—grounded in transformer-based language understanding, trained on diverse technical literature—to synthesize definitions, contrast methodologies, and contextualize research contributions.  \n",
      "Task Execution: Generated a comprehensive, layered explanation integrating conceptual clarity, comparative analysis, and citations—all in natural language, with no external file output.  \n",
      "\n",
      "✅ No file was generated — so no file path to report.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4e2ea",
   "metadata": {},
   "source": [
    "In the [LangSmith trace](https://smith.langchain.com/public/0322904b-bc4c-4433-a568-54c6b31bbef4/r/9ef1c23e-380e-46bf-94b3-d8bb33df440c) we can see the retrieved context incorporated into the model prompt.\n",
    "\n",
    "This is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\n",
    "\n",
    "**Returning source documents**\n",
    "The above [RAG chain](#rag-chains) incorporates retrieved context into a single system message for that run.\n",
    "\n",
    "As in the [agentic RAG](#rag-agents) formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n",
    "\n",
    "1. Adding a key to the state to store the retrieved documents\n",
    "2. Adding a new node via a [pre-model hook](https://docs.langchain.com/oss/python/langchain/agents#pre-model-hook) to populate that key (as well as inject the context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cfde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    context: list[Document]\n",
    "\n",
    "\n",
    "class RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n",
    "    state_schema = State\n",
    "\n",
    "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        retrieved_docs = vector_store.similarity_search(last_message.text)\n",
    "\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "        augmented_message_content = (\n",
    "            f\"{last_message.text}\\n\\n\"\n",
    "            \"Use the following context to answer the query:\\n\"\n",
    "            f\"{docs_content}\"\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n",
    "            \"context\": retrieved_docs,\n",
    "        }\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[],\n",
    "    middleware=[RetrieveDocumentsMiddleware()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5635f1e",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "\n",
    "Now that we've implemented a simple RAG application via [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), we can easily incorporate new features and go deeper:\n",
    "\n",
    "* [Stream](/oss/python/langchain/streaming) tokens and other information for responsive user experiences\n",
    "* Add [conversational memory](/oss/python/langchain/short-term-memory) to support multi-turn interactions\n",
    "* Add [long-term memory](/oss/python/langchain/long-term-memory) to support memory across conversational threads\n",
    "* Add [structured responses](/oss/python/langchain/structured-output)\n",
    "* Deploy your application with [LangSmith Deployment](/langsmith/deployments)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
